{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa41f66-4913-4896-bac9-64503ba89ff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Instructions\n",
    "- Attach notebook to a cluster (Serverless preferred).\n",
    "- Run Cells 2, 3 and 4 for the input widget parameters to get polpulated.\n",
    "- Fill in the parameters. Details on these parameters can be found [here](https://github.com/mohanab89/databricks-dashboard-suite#run-the-create_dashboards-notebook).\n",
    "- Run the rest of the notebook to deploy dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c01ba9c3-7432-4918-84bf-06cfde688ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-sdk==0.38.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a23c5dbe-1abd-43d7-a514-1931b103921f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import AccountClient\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import jobs\n",
    "from pyspark.errors import PySparkException\n",
    "\n",
    "# Client initialized for the current workspace\n",
    "w = WorkspaceClient()\n",
    "\n",
    "warehouses = w.warehouses.list()\n",
    "warehouse_names = [f\"{w.name} - ({w.id})**\" if w.enable_serverless_compute else f\"{w.name} - ({w.id})\" for w in warehouses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d9532d-61d9-4a57-ba5a-681edaec0514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.multiselect('actions', 'All', choices=['All', 'Deploy Dashboards', 'Publish Dashboards', 'Create Functions', 'Create/refresh Tables']) # Select the actions to perform\n",
    "dbutils.widgets.dropdown('warehouse', warehouse_names[0], choices=warehouse_names) # Select a warehouse which would be used in dashboards\n",
    "dbutils.widgets.text('catalog', 'main') # Provide a catalog where you have read/write permissions where required functions will be created. Catalog will be created if not found.\n",
    "dbutils.widgets.text('schema', 'default') # Provide a schema where you have read/write permissions where required functions will be created. Schema will be created if not found.\n",
    "dbutils.widgets.text('tags_to_consider_for_team_name', 'team_name,group') # Provide a comma separated list of tags that should be considered for getting team names\n",
    "dbutils.widgets.text('account_host', 'https://accounts.cloud.databricks.com/') # Provide the host for account console\n",
    "dbutils.widgets.text('account_id', '') # Provide the account identifier from account console\n",
    "dbutils.widgets.text('client_id', '') # Provide a M2M client ID for authenticating to account level access\n",
    "dbutils.widgets.text('client_secret', '') # Provide a M2M client secret for authenticating to account level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3c8547-cd3e-4246-9d5b-06a2c305cadc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "actions = dbutils.widgets.get('actions')\n",
    "catalog = dbutils.widgets.get('catalog')\n",
    "schema = dbutils.widgets.get('schema')\n",
    "account_host = dbutils.widgets.get('account_host')\n",
    "account_id = dbutils.widgets.get('account_id')\n",
    "client_id = dbutils.widgets.get('client_id')\n",
    "client_secret = dbutils.widgets.get('client_secret')\n",
    "tags_to_consider_for_team_name = dbutils.widgets.get('tags_to_consider_for_team_name')\n",
    "warehouse = dbutils.widgets.get('warehouse')\n",
    "warehouse_id = warehouse.split(\"(\")[1].split(\")\")[0]\n",
    "\n",
    "try:\n",
    "    spark.sql(f'USE CATALOG {catalog}')\n",
    "except PySparkException as ex:\n",
    "  if (ex.getErrorClass() == \"NO_SUCH_CATALOG_EXCEPTION\"):\n",
    "    spark.sql(f'CREATE CATALOG IF NOT EXISTS {catalog}')\n",
    "  else:\n",
    "    raise\n",
    "spark.sql(f'CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d53861bd-644d-4784-a9d2-6a9ee73fab86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "from databricks.sdk.service.dashboards import Dashboard\n",
    "\n",
    "# List all the JSON files from the current folder\n",
    "def list_json_dash_files():\n",
    "    notebook_folder = json.loads(\n",
    "        dbutils.notebook.entry_point.getDbutils().notebook().getContext().safeToJson()\n",
    "    )[\"attributes\"][\"notebook_path\"]\n",
    "    new_folder_name = \"dashboard_assess_dbx_costs\"\n",
    "    dashboard_save_path = (\n",
    "        f'{notebook_folder.rsplit(\"/\", 1)[0]}/dashboard_assess_dbx_costs'\n",
    "    )\n",
    "    Path(new_folder_name).mkdir(parents=True, exist_ok=True)\n",
    "    json_files = [\n",
    "        f for f in os.listdir(\".\") if os.path.isfile(f) and f.endswith(\".lvdash.json\")\n",
    "    ]\n",
    "    print(f\"Dashboard JSON files found: {json_files}\")\n",
    "    return json_files, dashboard_save_path\n",
    "\n",
    "\n",
    "# Deploy dashboards from json files\n",
    "def deploy_dashboards():\n",
    "    # Get all JSON files in the current folder\n",
    "    json_files, dashboard_save_path = list_json_dash_files()\n",
    "    dash_name_id_dict = {}\n",
    "\n",
    "    # Process each JSON\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, \"r\") as file:\n",
    "            data = file.read().rstrip()\n",
    "        replaced_data = data.replace(\"{catalog}\", catalog).replace(\"{schema}\", schema)\n",
    "\n",
    "        dash_name = json_file.split(\".\")[0]\n",
    "\n",
    "        try:\n",
    "            # Update if the dashboard exists\n",
    "            dashboard_id = w.workspace.get_status(\n",
    "                f\"{dashboard_save_path}/{json_file}\"\n",
    "            ).resource_id\n",
    "            curr_dash = w.lakeview.get(dashboard_id)\n",
    "            curr_dash.serialized_dashboard = replaced_data\n",
    "            dash_updated = w.lakeview.update(\n",
    "                dashboard_id=dashboard_id,\n",
    "                dashboard=curr_dash,\n",
    "            )\n",
    "            print(\n",
    "                f'Dashboard \"{dash_name}\" updated successfully at {dash_updated.create_time}'\n",
    "            )\n",
    "            dash_name_id_dict[dash_name] = dashboard_id\n",
    "        except Exception as e:\n",
    "            # Create a new dashboard if it doesn't exist\n",
    "            if \"doesn't exist\" in str(e):\n",
    "                new_dash = Dashboard(\n",
    "                    display_name=dash_name,\n",
    "                    parent_path=dashboard_save_path,\n",
    "                    serialized_dashboard=replaced_data,\n",
    "                    warehouse_id=warehouse_id)\n",
    "                dash_created = w.lakeview.create(dashboard=new_dash)\n",
    "                dashboard_id = dash_created.dashboard_id\n",
    "                print(\n",
    "                    f'Dashboard \"{dash_name}\" created successfully at {dash_created.create_time}'\n",
    "                )\n",
    "                dash_name_id_dict[dash_name] = dashboard_id\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    # Update URLs in the index dashboard\n",
    "    for json_file in json_files:\n",
    "        if \"Databricks\" in json_file:\n",
    "            with open(json_file, \"r\") as file:\n",
    "                data = file.read().rstrip()\n",
    "            replaced_data = data.replace(\"{catalog}\", catalog).replace(\n",
    "                \"{schema}\", schema\n",
    "            )\n",
    "\n",
    "            host_url = f\"https://{spark.conf.get('spark.databricks.workspaceUrl')}\"\n",
    "\n",
    "            for dash_name, dashboard_id in dash_name_id_dict.items():\n",
    "                str_to_replace = f\"**[{dash_name}](*)**\"\n",
    "                to_replace_with = f\"**[{dash_name}]({host_url}/dashboardsv3/{dashboard_id}/published)**\"\n",
    "                replaced_data = replaced_data.replace(\n",
    "                    f\"{str_to_replace}\", f\"{to_replace_with}\"\n",
    "                )\n",
    "\n",
    "            dashboard_id = w.workspace.get_status(\n",
    "                f\"{dashboard_save_path}/{json_file}\"\n",
    "            ).resource_id\n",
    "            curr_dash = w.lakeview.get(dashboard_id)\n",
    "            curr_dash.serialized_dashboard = replaced_data\n",
    "            dash_updated = w.lakeview.update(\n",
    "                dashboard_id=dashboard_id,\n",
    "                dashboard=curr_dash,\n",
    "            )\n",
    "            print(\n",
    "                f'Dashboard \"{dash_name}\" updated successfully at {dash_updated.create_time} with links to other dashboards'\n",
    "            )\n",
    "            print(f\"{host_url}/dashboardsv3/{dashboard_id}/published\")\n",
    "            break\n",
    "\n",
    "\n",
    "# Publish dashboards\n",
    "def publish_dashboards():\n",
    "    json_files, dashboard_save_path = list_json_dash_files()\n",
    "\n",
    "    for json_file in json_files:\n",
    "        dash_name = json_file.split(\".\")[0]\n",
    "\n",
    "        try:\n",
    "            dashboard_id = w.workspace.get_status(\n",
    "                f\"{dashboard_save_path}/{json_file}\"\n",
    "            ).resource_id\n",
    "            dash_published = w.lakeview.publish(\n",
    "                dashboard_id=dashboard_id, warehouse_id=warehouse_id\n",
    "            )\n",
    "            print(\n",
    "                f'Dashboard \"{dash_name}\" published successfully at {dash_published.revision_create_time}'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Dashboard {dash_name} could not be published\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "def create_sql_functions():\n",
    "    # Create function to extract job type from SKU\n",
    "    print(f\"Creating {catalog}.{schema}.job_type_from_sku function...\")\n",
    "    spark.sql(\n",
    "        f\"\"\"CREATE OR REPLACE FUNCTION {catalog}.{schema}.job_type_from_sku(sku STRING)\n",
    "          RETURNS STRING\n",
    "          RETURN\n",
    "          CASE\n",
    "            WHEN sku LIKE '%JOBS_SERVERLESS%' THEN 'JOBS_SERVERLESS'\n",
    "            WHEN sku LIKE '%JOBS_COMPUTE_(PHOTON)%' THEN 'JOBS_COMPUTE_PHOTON'\n",
    "            WHEN sku LIKE '%JOBS_COMPUTE%' THEN 'JOBS_COMPUTE'\n",
    "            WHEN sku IS NULL THEN 'UNKNOWN'\n",
    "            ELSE 'OTHER'\n",
    "          END;\"\"\"\n",
    "    )\n",
    "    print(f\"Function {catalog}.{schema}.job_type_from_sku created successfully\")\n",
    "\n",
    "    # Create function to extract SQL type from SKU\n",
    "    print(f\"Creating {catalog}.{schema}.sql_type_from_sku function...\")\n",
    "    spark.sql(\n",
    "        f\"\"\"CREATE OR REPLACE FUNCTION {catalog}.{schema}.sql_type_from_sku(sku STRING)\n",
    "          RETURNS STRING\n",
    "          RETURN\n",
    "          CASE\n",
    "            WHEN sku LIKE '%SERVERLESS_SQL%' THEN 'SQL_SERVERLESS'\n",
    "            WHEN sku LIKE '%SQL_PRO%' THEN 'SQL_PRO'\n",
    "            WHEN sku LIKE '%SQL%' THEN 'SQL_CLASSIC'\n",
    "            WHEN sku IS NULL THEN 'UNKNOWN'\n",
    "            ELSE 'OTHER'\n",
    "          END;\"\"\"\n",
    "    )\n",
    "    print(f\"Function {catalog}.{schema}.sql_type_from_sku created successfully\")\n",
    "\n",
    "    # Programatically create function to extract team name from the tags using input parameter\n",
    "    print(f\"Creating {catalog}.{schema}.team_name_from_tags function...\")\n",
    "    keys = tags_to_consider_for_team_name.split(\",\")\n",
    "    param_cols = [\"cluster_tags\", \"job_tags\"]\n",
    "\n",
    "    # Dynamically construct the CASE statement\n",
    "    case_list = []\n",
    "    for each_param_col in param_cols:\n",
    "        case_statement = \"CASE  \\n\"\n",
    "        for key in keys:\n",
    "            case_statement += f\"WHEN map_contains_key({each_param_col}, '{key.strip()}') THEN lower({each_param_col}.{key.strip()}) \\n\"\n",
    "        case_statement += f\"WHEN map_contains_key({each_param_col}, 'LakehouseMonitoring') AND {each_param_col}.LakehouseMonitoring = 'true' THEN 'LakehouseMonitoring' \\n\"\n",
    "        case_statement += f\"ELSE NULL END AS {each_param_col}_team_name_init \\n\"\n",
    "        case_list.append(case_statement)\n",
    "\n",
    "    query = f\"SELECT ifnull({param_cols[0]}_team_name_init, {param_cols[1]}_team_name_init) as team_name_init FROM \\n (SELECT {', '.join(case_list)})\"\n",
    "\n",
    "    # Final SQL query\n",
    "    query = f\"(SELECT ifnull(team_name_init, 'unknown') AS team_name FROM \\n ({query}))\"\n",
    "\n",
    "    # Create the function\n",
    "    spark.sql(\n",
    "        f\"\"\"create or replace function {catalog}.{schema}.team_name_from_tags(cluster_tags MAP<STRING,STRING>, job_tags MAP<STRING,STRING>)\n",
    "        RETURNS STRING RETURN {query}\"\"\"\n",
    "    )\n",
    "    print(f\"Function {catalog}.{schema}.team_name_from_tags created successfully\")\n",
    "\n",
    "\n",
    "def create_update_tables():\n",
    "    # Programatically create/update a table for workspace id to name mapping\n",
    "    print(f\"Creating {catalog}.{schema}.workspace_reference table...\")\n",
    "    spark.sql(\n",
    "        f\"CREATE TABLE IF NOT EXISTS {catalog}.{schema}.workspace_reference (workspace_id STRING, workspace_name STRING)\"\n",
    "    )\n",
    "    try:\n",
    "        a = AccountClient(\n",
    "            host=account_host,\n",
    "            account_id=account_id,\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "        )\n",
    "        workspaces = [\n",
    "            [workspace.workspace_id, workspace.workspace_name]\n",
    "            for workspace in a.workspaces.list()\n",
    "        ]\n",
    "        workspace_ref = spark.createDataFrame(\n",
    "            workspaces, [\"workspace_id\", \"workspace_name\"]\n",
    "        )\n",
    "        workspace_ref = workspace_ref.withColumn(\"workspace_id\", col(\"workspace_id\").cast(StringType()))\n",
    "        workspace_ref.createOrReplaceTempView('workspace_ref')\n",
    "        workspace_ref = spark.sql(\n",
    "            f\"\"\"SELECT * FROM workspace_ref\n",
    "            UNION\n",
    "            select distinct workspace_id, workspace_id as workspace_name from system.billing.usage\n",
    "            WHERE workspace_id NOT IN (SELECT workspace_id FROM workspace_ref)\"\"\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"\\t Creating default workspace_reference table.\")\n",
    "        workspace_ref = spark.sql(\n",
    "            f\"\"\"SELECT * FROM {catalog}.{schema}.workspace_reference\n",
    "            UNION\n",
    "            select distinct workspace_id, workspace_id as workspace_name from system.billing.usage\n",
    "            WHERE workspace_id NOT IN (SELECT workspace_id FROM {catalog}.{schema}.workspace_reference)\"\"\"\n",
    "        )\n",
    "\n",
    "    workspace_ref.write.mode(\"overwrite\").saveAsTable(\n",
    "        f\"{catalog}.{schema}.workspace_reference\"\n",
    "    )\n",
    "    print(f\"Table {catalog}.{schema}.workspace_reference created/updated successfully\")\n",
    "\n",
    "    # Programatically create/update a table for warehouse id to name mapping\n",
    "    print(f\"Creating {catalog}.{schema}.warehouse_reference table...\")\n",
    "    spark.sql(\n",
    "        f\"CREATE TABLE IF NOT EXISTS {catalog}.{schema}.warehouse_reference (workspace_id STRING, warehouse_id STRING, warehouse_name STRING)\"\n",
    "    )\n",
    "    warehouse_names = spark.sql(\n",
    "        f\"\"\"WITH warehouse_names AS (\n",
    "              SELECT\n",
    "                workspace_id,\n",
    "                GET_JSON_OBJECT(response.result, '$.id') AS warehouse_id,\n",
    "                max(request_params.name) AS warehouse_name\n",
    "              from\n",
    "                system.access.audit\n",
    "              WHERE\n",
    "                service_name = 'databrickssql'\n",
    "              group by\n",
    "                workspace_id,\n",
    "                GET_JSON_OBJECT(response.result, '$.id')\n",
    "            ),\n",
    "            union_warehouses AS (\n",
    "              SELECT * FROM warehouse_names\n",
    "              UNION\n",
    "              SELECT * FROM {catalog}.{schema}.warehouse_reference\n",
    "            )\n",
    "            SELECT * FROM union_warehouses\"\"\"\n",
    "    )\n",
    "\n",
    "    warehouse_names.write.mode(\"overwrite\").saveAsTable(\n",
    "        f\"{catalog}.{schema}.warehouse_reference\"\n",
    "    )\n",
    "    print(f\"Table {catalog}.{schema}.warehouse_reference created/updated successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22dd2e0-3a8f-49a9-ba59-515af3a97d34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_actions = actions.split(\",\")\n",
    "for each_action in all_actions:\n",
    "    if each_action == \"Deploy Dashboards\":\n",
    "        print(\"Deploying dashboards...\")\n",
    "        deploy_dashboards()\n",
    "    elif each_action == \"Publish Dashboards\":\n",
    "        print(\"Publishing dashboards...\")\n",
    "        publish_dashboards()\n",
    "    elif each_action == \"Create Functions\":\n",
    "        print(\"Creating functions...\")\n",
    "        create_sql_functions()\n",
    "    elif each_action == \"Create/refresh Tables\":\n",
    "        print(\"Creating/refreshing tables...\")\n",
    "        create_update_tables()\n",
    "    else:\n",
    "        print(f\"Performing all actions...\")\n",
    "        deploy_dashboards()\n",
    "        publish_dashboards()\n",
    "        create_sql_functions()\n",
    "        create_update_tables()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3270557957991702,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "create_dashboards",
   "widgets": {
    "account_host": {
     "currentValue": "https://accounts.cloud.databricks.com/",
     "nuid": "dd9f81a7-b55b-4862-a447-793fe8488813",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "https://accounts.cloud.databricks.com/",
      "label": null,
      "name": "account_host",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "https://accounts.cloud.databricks.com/",
      "label": null,
      "name": "account_host",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "account_id": {
     "currentValue": "",
     "nuid": "d68f7ca7-45d8-4025-8236-6b79cb15e9f9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "account_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "account_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "actions": {
     "currentValue": "All",
     "nuid": "6af7996e-e289-4118-ac21-4d59ab5ff1b0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "All",
      "label": null,
      "name": "actions",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "All",
        "Deploy Dashboards",
        "Publish Dashboards",
        "Create Functions",
        "Create/refresh Tables"
       ],
       "fixedDomain": true,
       "multiselect": true
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "multiselect",
      "defaultValue": "All",
      "label": null,
      "name": "actions",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "All",
        "Deploy Dashboards",
        "Publish Dashboards",
        "Create Functions",
        "Create/refresh Tables"
       ]
      }
     }
    },
    "catalog": {
     "currentValue": "main",
     "nuid": "a276281f-0659-42d2-81e7-ccdbdda915b9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "main",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "main",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "client_id": {
     "currentValue": "",
     "nuid": "51cc571e-8dab-4db2-8ee0-74381bbf8747",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "client_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "client_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "client_secret": {
     "currentValue": "",
     "nuid": "9efac38f-aee3-44e6-b119-93da09664c98",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "client_secret",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "client_secret",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "default",
     "nuid": "f07dca19-6ff5-41fd-b144-cab808be3517",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": null,
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": null,
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "tags_to_consider_for_team_name": {
     "currentValue": "team_name,group",
     "nuid": "4f134aae-7386-42c5-a24f-da17757e36c9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "team_name,group",
      "label": null,
      "name": "tags_to_consider_for_team_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "team_name,group",
      "label": null,
      "name": "tags_to_consider_for_team_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "warehouse": {
     "currentValue": "--Select Warehouse--",
     "nuid": "8867c7b8-4fc7-4a1f-a2cf-97469c379663",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "--Select Warehouse--",
      "label": null,
      "name": "warehouse",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "--Select Warehouse--"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "--Select Warehouse--",
      "label": null,
      "name": "warehouse",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "--Select Warehouse--"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
