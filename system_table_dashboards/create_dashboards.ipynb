{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa41f66-4913-4896-bac9-64503ba89ff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Instrucciones\n",
    "- Adjunta el notebook a un cluster (Serverless es preferido).\n",
    "- Ejecuta las Celdas 2, 3 y 4 para que los parámetros de widgets se pueblen.\n",
    "- Completa los parámetros según la descripción a continuación.\n",
    "- Ejecuta el resto del notebook para desplegar los dashboards.\n",
    "\n",
    "### Parámetros del Notebook\n",
    "\n",
    "**Parámetros Principales:**\n",
    "- **`actions`**: Selecciona las acciones a realizar durante el despliegue. Para el primer despliegue, selecciona **All** para ejecutar todas las acciones (Deploy Dashboards, Publish Dashboards, Create Functions, Create/Refresh Tables).\n",
    "- **`warehouse`**: Selecciona un SQL warehouse que será usado por los dashboards para ejecutar queries. Los warehouses serverless están marcados con \\*\\* y son recomendados.\n",
    "- **`catalog`**: Especifica un catálogo UC donde tienes permisos de lectura/escritura. Aquí se crearán las tablas y funciones necesarias. Se creará automáticamente si no existe.\n",
    "- **`schema`**: Proporciona el nombre del schema dentro del catálogo seleccionado donde se almacenarán las tablas y funciones. Se creará automáticamente si no existe.\n",
    "- **`workspace_name`**: Nombre descriptivo del workspace actual. Este nombre se usará en los dashboards para visualización en lugar del workspace ID.\n",
    "- **`tags_to_consider_for_team_name`**: Lista separada por comas de claves de tags que se usarán para identificar equipos en los dashboards (ej: `team_name,group`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c01ba9c3-7432-4918-84bf-06cfde688ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-sdk==0.38.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a23c5dbe-1abd-43d7-a514-1931b103921f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import jobs\n",
    "from pyspark.errors import PySparkException\n",
    "\n",
    "# Cliente inicializado para el workspace actual\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Obtener lista de warehouses disponibles\n",
    "warehouses = w.warehouses.list()\n",
    "warehouse_names = [f\"{w.name} - ({w.id})**\" if w.enable_serverless_compute else f\"{w.name} - ({w.id})\" for w in warehouses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d9532d-61d9-4a57-ba5a-681edaec0514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Limpiar todos los widgets existentes antes de crearlos\n",
    "dbutils.widgets.removeAll()\n",
    "\n",
    "dbutils.widgets.multiselect('actions', 'All', choices=['All', 'Deploy Dashboards', 'Publish Dashboards', 'Create Functions', 'Create/refresh Tables']) # Selecciona las acciones a realizar\n",
    "dbutils.widgets.dropdown('warehouse', warehouse_names[0], choices=warehouse_names) # Selecciona un warehouse que será usado en los dashboards\n",
    "dbutils.widgets.text('catalog', 'main') # Proporciona un catálogo donde tienes permisos de lectura/escritura. El catálogo se creará si no existe.\n",
    "dbutils.widgets.text('schema', 'default') # Proporciona un schema donde tienes permisos de lectura/escritura. El schema se creará si no existe.\n",
    "dbutils.widgets.text('workspace_name', 'Mi Workspace') # Proporciona un nombre descriptivo para el workspace actual\n",
    "dbutils.widgets.text('tags_to_consider_for_team_name', 'team_name,group') # Proporciona una lista separada por comas de tags para identificar equipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3c8547-cd3e-4246-9d5b-06a2c305cadc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "actions = dbutils.widgets.get('actions')\n",
    "catalog = dbutils.widgets.get('catalog')\n",
    "schema = dbutils.widgets.get('schema')\n",
    "workspace_name = dbutils.widgets.get('workspace_name')\n",
    "tags_to_consider_for_team_name = dbutils.widgets.get('tags_to_consider_for_team_name')\n",
    "warehouse = dbutils.widgets.get('warehouse')\n",
    "warehouse_id = warehouse.split(\"(\")[1].split(\")\")[0]\n",
    "\n",
    "try:\n",
    "    spark.sql(f'USE CATALOG {catalog}')\n",
    "except PySparkException as ex:\n",
    "  if (ex.getErrorClass() == \"NO_SUCH_CATALOG_EXCEPTION\"):\n",
    "    spark.sql(f'CREATE CATALOG IF NOT EXISTS {catalog}')\n",
    "  else:\n",
    "    raise\n",
    "spark.sql(f'CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d53861bd-644d-4784-a9d2-6a9ee73fab86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "from databricks.sdk.service.dashboards import Dashboard\n",
    "\n",
    "# Lista todos los archivos JSON de la carpeta actual\n",
    "def list_json_dash_files():\n",
    "    notebook_folder = json.loads(\n",
    "        dbutils.notebook.entry_point.getDbutils().notebook().getContext().safeToJson()\n",
    "    )[\"attributes\"][\"notebook_path\"]\n",
    "    new_folder_name = \"dashboard_assess_dbx_costs\"\n",
    "    dashboard_save_path = (\n",
    "        f'{notebook_folder.rsplit(\"/\", 1)[0]}/dashboard_assess_dbx_costs'\n",
    "    )\n",
    "    Path(new_folder_name).mkdir(parents=True, exist_ok=True)\n",
    "    json_files = [\n",
    "        f for f in os.listdir(\".\") if os.path.isfile(f) and f.endswith(\".lvdash.json\")\n",
    "    ]\n",
    "    print(f\"Dashboard JSON files found: {json_files}\")\n",
    "    return json_files, dashboard_save_path\n",
    "\n",
    "\n",
    "# Despliega dashboards desde archivos json\n",
    "def deploy_dashboards():\n",
    "    # Obtiene todos los archivos JSON de la carpeta actual\n",
    "    json_files, dashboard_save_path = list_json_dash_files()\n",
    "    dash_name_id_dict = {}\n",
    "\n",
    "    # Procesa cada JSON\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, \"r\") as file:\n",
    "            data = file.read().rstrip()\n",
    "        replaced_data = data.replace(\"{catalog}\", catalog).replace(\"{schema}\", schema)\n",
    "\n",
    "        dash_name = json_file.split(\".\")[0]\n",
    "\n",
    "        try:\n",
    "            # Actualiza el dashboard si ya existe\n",
    "            dashboard_id = w.workspace.get_status(\n",
    "                f\"{dashboard_save_path}/{json_file}\"\n",
    "            ).resource_id\n",
    "            curr_dash = w.lakeview.get(dashboard_id)\n",
    "            curr_dash.serialized_dashboard = replaced_data\n",
    "            dash_updated = w.lakeview.update(\n",
    "                dashboard_id=dashboard_id,\n",
    "                dashboard=curr_dash,\n",
    "            )\n",
    "            print(\n",
    "                f'Dashboard \"{dash_name}\" updated successfully at {dash_updated.create_time}'\n",
    "            )\n",
    "            dash_name_id_dict[dash_name] = dashboard_id\n",
    "        except Exception as e:\n",
    "            # Crea un nuevo dashboard si no existe\n",
    "            if \"doesn't exist\" in str(e):\n",
    "                new_dash = Dashboard(\n",
    "                    display_name=dash_name,\n",
    "                    parent_path=dashboard_save_path,\n",
    "                    serialized_dashboard=replaced_data,\n",
    "                    warehouse_id=warehouse_id)\n",
    "                dash_created = w.lakeview.create(dashboard=new_dash)\n",
    "                dashboard_id = dash_created.dashboard_id\n",
    "                print(\n",
    "                    f'Dashboard \"{dash_name}\" created successfully at {dash_created.create_time}'\n",
    "                )\n",
    "                dash_name_id_dict[dash_name] = dashboard_id\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    # Actualiza URLs en el dashboard índice\n",
    "    for json_file in json_files:\n",
    "        if \"Databricks\" in json_file:\n",
    "            with open(json_file, \"r\") as file:\n",
    "                data = file.read().rstrip()\n",
    "            replaced_data = data.replace(\"{catalog}\", catalog).replace(\n",
    "                \"{schema}\", schema\n",
    "            )\n",
    "\n",
    "            host_url = f\"https://{spark.conf.get('spark.databricks.workspaceUrl')}\"\n",
    "\n",
    "            for dash_name, dashboard_id in dash_name_id_dict.items():\n",
    "                str_to_replace = f\"**[{dash_name}](*)**\"\n",
    "                to_replace_with = f\"**[{dash_name}]({host_url}/dashboardsv3/{dashboard_id}/published)**\"\n",
    "                replaced_data = replaced_data.replace(\n",
    "                    f\"{str_to_replace}\", f\"{to_replace_with}\"\n",
    "                )\n",
    "\n",
    "            dashboard_id = w.workspace.get_status(\n",
    "                f\"{dashboard_save_path}/{json_file}\"\n",
    "            ).resource_id\n",
    "            curr_dash = w.lakeview.get(dashboard_id)\n",
    "            curr_dash.serialized_dashboard = replaced_data\n",
    "            dash_updated = w.lakeview.update(\n",
    "                dashboard_id=dashboard_id,\n",
    "                dashboard=curr_dash,\n",
    "            )\n",
    "            print(\n",
    "                f'Dashboard \"{dash_name}\" updated successfully at {dash_updated.create_time} with links to other dashboards'\n",
    "            )\n",
    "            print(f\"{host_url}/dashboardsv3/{dashboard_id}/published\")\n",
    "            break\n",
    "\n",
    "\n",
    "# Publica dashboards\n",
    "def publish_dashboards():\n",
    "    json_files, dashboard_save_path = list_json_dash_files()\n",
    "\n",
    "    for json_file in json_files:\n",
    "        dash_name = json_file.split(\".\")[0]\n",
    "\n",
    "        try:\n",
    "            dashboard_id = w.workspace.get_status(\n",
    "                f\"{dashboard_save_path}/{json_file}\"\n",
    "            ).resource_id\n",
    "            dash_published = w.lakeview.publish(\n",
    "                dashboard_id=dashboard_id, warehouse_id=warehouse_id\n",
    "            )\n",
    "            print(\n",
    "                f'Dashboard \"{dash_name}\" published successfully at {dash_published.revision_create_time}'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Dashboard {dash_name} could not be published\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "def create_sql_functions():\n",
    "    # Crea función para extraer el tipo de job desde el SKU\n",
    "    print(f\"Creating {catalog}.{schema}.job_type_from_sku function...\")\n",
    "    spark.sql(\n",
    "        f\"\"\"CREATE OR REPLACE FUNCTION {catalog}.{schema}.job_type_from_sku(sku STRING)\n",
    "          RETURNS STRING\n",
    "          RETURN\n",
    "          CASE\n",
    "            WHEN sku LIKE '%JOBS_SERVERLESS%' THEN 'JOBS_SERVERLESS'\n",
    "            WHEN sku LIKE '%JOBS_COMPUTE_(PHOTON)%' THEN 'JOBS_COMPUTE_PHOTON'\n",
    "            WHEN sku LIKE '%JOBS_COMPUTE%' THEN 'JOBS_COMPUTE'\n",
    "            WHEN sku IS NULL THEN 'UNKNOWN'\n",
    "            ELSE 'OTHER'\n",
    "          END;\"\"\"\n",
    "    )\n",
    "    print(f\"Function {catalog}.{schema}.job_type_from_sku created successfully\")\n",
    "\n",
    "    # Crea función para extraer el tipo de SQL desde el SKU\n",
    "    print(f\"Creating {catalog}.{schema}.sql_type_from_sku function...\")\n",
    "    spark.sql(\n",
    "        f\"\"\"CREATE OR REPLACE FUNCTION {catalog}.{schema}.sql_type_from_sku(sku STRING)\n",
    "          RETURNS STRING\n",
    "          RETURN\n",
    "          CASE\n",
    "            WHEN sku LIKE '%SERVERLESS_SQL%' THEN 'SQL_SERVERLESS'\n",
    "            WHEN sku LIKE '%SQL_PRO%' THEN 'SQL_PRO'\n",
    "            WHEN sku LIKE '%SQL%' THEN 'SQL_CLASSIC'\n",
    "            WHEN sku IS NULL THEN 'UNKNOWN'\n",
    "            ELSE 'OTHER'\n",
    "          END;\"\"\"\n",
    "    )\n",
    "    print(f\"Function {catalog}.{schema}.sql_type_from_sku created successfully\")\n",
    "\n",
    "    # Crea función programáticamente para extraer el nombre del equipo desde los tags usando el parámetro de entrada\n",
    "    print(f\"Creating {catalog}.{schema}.team_name_from_tags function...\")\n",
    "    keys = [] if tags_to_consider_for_team_name == '' else tags_to_consider_for_team_name.split(\",\")\n",
    "    param_cols = [\"cluster_tags\", \"job_tags\"]\n",
    "\n",
    "    # Construye dinámicamente el statement CASE\n",
    "    case_list = []\n",
    "    for each_param_col in param_cols:\n",
    "        case_statement = \"CASE  \\n\"\n",
    "        if len(keys) > 0:\n",
    "            for key in keys:\n",
    "                case_statement += f\"WHEN map_contains_key({each_param_col}, '{key.strip()}') THEN lower({each_param_col}.`{key.strip()}`) \\n\"\n",
    "        case_statement += f\"WHEN map_contains_key({each_param_col}, 'LakehouseMonitoring') AND {each_param_col}.LakehouseMonitoring = 'true' THEN 'LakehouseMonitoring' \\n\"\n",
    "        case_statement += f\"ELSE NULL END AS {each_param_col}_team_name_init \\n\"\n",
    "        case_list.append(case_statement)\n",
    "\n",
    "    query = f\"SELECT ifnull({param_cols[0]}_team_name_init, {param_cols[1]}_team_name_init) as team_name_init FROM \\n (SELECT {', '.join(case_list)})\"\n",
    "\n",
    "    # Query SQL final\n",
    "    query = f\"(SELECT ifnull(team_name_init, 'unknown') AS team_name FROM \\n ({query}))\"\n",
    "\n",
    "    # Crea la función\n",
    "    spark.sql(\n",
    "        f\"\"\"create or replace function {catalog}.{schema}.team_name_from_tags(cluster_tags MAP<STRING,STRING>, job_tags MAP<STRING,STRING>)\n",
    "        RETURNS STRING RETURN {query}\"\"\"\n",
    "    )\n",
    "    print(f\"Function {catalog}.{schema}.team_name_from_tags created successfully\")\n",
    "\n",
    "\n",
    "def create_update_tables():\n",
    "    # Crea/actualiza programáticamente una tabla para mapeo de workspace id a nombre\n",
    "    print(f\"Creating {catalog}.{schema}.workspace_reference table...\")\n",
    "    # CREATE OR REPLACE limpia la tabla automáticamente en cada ejecución\n",
    "    spark.sql(\n",
    "        f\"CREATE OR REPLACE TABLE {catalog}.{schema}.workspace_reference (workspace_id STRING, workspace_name STRING)\"\n",
    "    )\n",
    "    \n",
    "    # Obtiene el ID del workspace actual usando múltiples métodos\n",
    "    current_workspace_id = None\n",
    "    \n",
    "    # Método 1: Extraer del workspace URL (más confiable)\n",
    "    try:\n",
    "        workspace_url = spark.conf.get('spark.databricks.workspaceUrl')\n",
    "        # URL format: adb-WORKSPACE_ID.REGION.azuredatabricks.net o dbc-WORKSPACE_ID.cloud.databricks.com\n",
    "        if 'adb-' in workspace_url:\n",
    "            current_workspace_id = workspace_url.split('adb-')[1].split('.')[0]\n",
    "        elif 'dbc-' in workspace_url:\n",
    "            current_workspace_id = workspace_url.split('dbc-')[1].split('.')[0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Método 2: Desde la configuración de spark\n",
    "    if not current_workspace_id:\n",
    "        try:\n",
    "            current_workspace_id = spark.conf.get(\"spark.databricks.workspaceId\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Método 3: Desde system.billing.usage (solo workspace actual)\n",
    "    if not current_workspace_id:\n",
    "        try:\n",
    "            current_workspace_id = spark.sql(\"SELECT DISTINCT workspace_id FROM system.billing.usage LIMIT 1\").collect()[0][0]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Fallback final\n",
    "    if not current_workspace_id:\n",
    "        current_workspace_id = \"unknown\"\n",
    "    \n",
    "    # Inserta la información del workspace actual en la tabla limpia\n",
    "    print(f\"\\t Using workspace_id: {current_workspace_id}, workspace_name: {workspace_name}\")\n",
    "    spark.sql(\n",
    "        f\"\"\"INSERT INTO {catalog}.{schema}.workspace_reference \n",
    "        VALUES ('{current_workspace_id}', '{workspace_name}')\"\"\"\n",
    "    )\n",
    "    print(f\"Table {catalog}.{schema}.workspace_reference created/updated successfully\")\n",
    "\n",
    "    # Crea/actualiza programáticamente una tabla para mapeo de warehouse id a nombre\n",
    "    print(f\"Creating {catalog}.{schema}.warehouse_reference table...\")\n",
    "    # CREATE OR REPLACE limpia la tabla automáticamente en cada ejecución\n",
    "    spark.sql(\n",
    "        f\"CREATE OR REPLACE TABLE {catalog}.{schema}.warehouse_reference (workspace_id STRING, warehouse_id STRING, warehouse_name STRING)\"\n",
    "    )\n",
    "    \n",
    "    # Obtiene los warehouses SOLO del workspace actual combinando dos fuentes:\n",
    "    # 1. system.compute.warehouses - warehouses actuales y su configuración oficial\n",
    "    # 2. system.access.audit - warehouses históricos que ya no existen pero tienen datos de billing\n",
    "    spark.sql(\n",
    "        f\"\"\"INSERT INTO {catalog}.{schema}.warehouse_reference\n",
    "            SELECT\n",
    "              workspace_id,\n",
    "              warehouse_id,\n",
    "              MAX(warehouse_name) as warehouse_name\n",
    "            FROM\n",
    "              (\n",
    "                -- Warehouses actuales desde system.compute.warehouses\n",
    "                SELECT\n",
    "                  workspace_id,\n",
    "                  warehouse_id,\n",
    "                  warehouse_name\n",
    "                FROM\n",
    "                  (\n",
    "                    SELECT\n",
    "                      workspace_id,\n",
    "                      warehouse_id,\n",
    "                      warehouse_name,\n",
    "                      delete_time,\n",
    "                      ROW_NUMBER() OVER(\n",
    "                        PARTITION BY workspace_id, warehouse_id\n",
    "                        ORDER BY change_time DESC\n",
    "                      ) as rn\n",
    "                    FROM\n",
    "                      system.compute.warehouses\n",
    "                    WHERE\n",
    "                      workspace_id = '{current_workspace_id}'\n",
    "                  )\n",
    "                WHERE\n",
    "                  rn = 1\n",
    "                  AND delete_time IS NULL\n",
    "                \n",
    "                UNION\n",
    "                \n",
    "                -- Warehouses históricos desde system.access.audit\n",
    "                SELECT\n",
    "                  workspace_id,\n",
    "                  GET_JSON_OBJECT(response.result, '$.id') AS warehouse_id,\n",
    "                  request_params.name AS warehouse_name\n",
    "                FROM\n",
    "                  system.access.audit\n",
    "                WHERE\n",
    "                  service_name = 'databrickssql'\n",
    "                  AND GET_JSON_OBJECT(response.result, '$.id') IS NOT NULL\n",
    "                  AND workspace_id = '{current_workspace_id}'\n",
    "              )\n",
    "            GROUP BY\n",
    "              workspace_id,\n",
    "              warehouse_id\"\"\"\n",
    "    )\n",
    "    print(f\"Table {catalog}.{schema}.warehouse_reference created/updated successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22dd2e0-3a8f-49a9-ba59-515af3a97d34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_actions = actions.split(\",\")\n",
    "for each_action in all_actions:\n",
    "    if each_action == \"Deploy Dashboards\":\n",
    "        print(\"Deploying dashboards...\")\n",
    "        deploy_dashboards()\n",
    "    elif each_action == \"Publish Dashboards\":\n",
    "        print(\"Publishing dashboards...\")\n",
    "        publish_dashboards()\n",
    "    elif each_action == \"Create Functions\":\n",
    "        print(\"Creating functions...\")\n",
    "        create_sql_functions()\n",
    "    elif each_action == \"Create/refresh Tables\":\n",
    "        print(\"Creating/refreshing tables...\")\n",
    "        create_update_tables()\n",
    "    else:\n",
    "        print(f\"Performing all actions...\")\n",
    "        deploy_dashboards()\n",
    "        publish_dashboards()\n",
    "        create_sql_functions()\n",
    "        create_update_tables()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3270557957991702,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "create_dashboards",
   "widgets": {
    "account_host": {
     "currentValue": "https://accounts.cloud.databricks.com/",
     "nuid": "dd9f81a7-b55b-4862-a447-793fe8488813",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "https://accounts.cloud.databricks.com/",
      "label": null,
      "name": "account_host",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "https://accounts.cloud.databricks.com/",
      "label": null,
      "name": "account_host",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "account_id": {
     "currentValue": "",
     "nuid": "d68f7ca7-45d8-4025-8236-6b79cb15e9f9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "account_id",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "account_id",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "actions": {
     "currentValue": "All",
     "nuid": "6af7996e-e289-4118-ac21-4d59ab5ff1b0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "All",
      "label": null,
      "name": "actions",
      "options": {
       "choices": [
        "All",
        "Deploy Dashboards",
        "Publish Dashboards",
        "Create Functions",
        "Create/refresh Tables"
       ],
       "fixedDomain": true,
       "multiselect": true,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "All",
      "label": null,
      "name": "actions",
      "options": {
       "autoCreated": null,
       "choices": [
        "All",
        "Deploy Dashboards",
        "Publish Dashboards",
        "Create Functions",
        "Create/refresh Tables"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "multiselect"
     }
    },
    "catalog": {
     "currentValue": "main",
     "nuid": "a276281f-0659-42d2-81e7-ccdbdda915b9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "main",
      "label": null,
      "name": "catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "main",
      "label": null,
      "name": "catalog",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "client_id": {
     "currentValue": "",
     "nuid": "51cc571e-8dab-4db2-8ee0-74381bbf8747",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "client_id",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "client_id",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "client_secret": {
     "currentValue": "",
     "nuid": "9efac38f-aee3-44e6-b119-93da09664c98",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "client_secret",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "client_secret",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "schema": {
     "currentValue": "default",
     "nuid": "f07dca19-6ff5-41fd-b144-cab808be3517",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": null,
      "name": "schema",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "default",
      "label": null,
      "name": "schema",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "tags_to_consider_for_team_name": {
     "currentValue": "team_name,group",
     "nuid": "4f134aae-7386-42c5-a24f-da17757e36c9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "team_name,group",
      "label": null,
      "name": "tags_to_consider_for_team_name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "team_name,group",
      "label": null,
      "name": "tags_to_consider_for_team_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "warehouse": {
     "currentValue": "--Select Warehouse--",
     "nuid": "8867c7b8-4fc7-4a1f-a2cf-97469c379663",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "--Select Warehouse--",
      "label": null,
      "name": "warehouse",
      "options": {
       "choices": [
        "--Select Warehouse--"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "--Select Warehouse--",
      "label": null,
      "name": "warehouse",
      "options": {
       "autoCreated": null,
       "choices": [
        "--Select Warehouse--"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
